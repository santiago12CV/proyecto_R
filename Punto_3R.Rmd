---
title: "Dominick's Finer Foods"
author: "Duvan Santiago Castro - Miguel Ángel Malagón"
date: '2022-10-04'
output:
  html_document:
    df_print: paged
---

# Introducción

A lo largo de las siguientes secciones encontrará los diferentes modelos estimados y sus resultados correspondientes, que permitirá entender la demanda de cerveza de la cadena de restaurantes Dominick’s Finer Foods (DFF). Para dicha interpretación nos centraremos en 2 tipos de cervezas con sus respectivos precios.

Inicialmente realizaremos una serie de filtros que nos permitan seleccionar una muestra de 200 observaciones para el desarrollo de los modelos. 


# Primeros pasos - Alistamiento datos 

Para la extracción de los datos, vamos a utilizar la base de datos wber, provenientes de (DFF) y les aplicaremos los siguientes filtros:

  - Store = 109: Nos centraremos en la tienda 109
  - Precio > 0
  - Unidades > 0
  - Seleccionar 2 tipos de marcas:
      - BUDWEISER BEER
      - BUDWEISER LIGHT BEER
```{r,warning=FALSE}
#intalacion de las librerías necesarias

library(dplyr)
library(tidyverse)
```



```{r}
datos_cervezas <- read.csv(file.choose())
```

```{r}
beer <- filter(datos_cervezas,STORE==109,(MOVE>0& PRICE>0),(UPC==1820000016|UPC==1820000106))

```

```{r}
# Cambio los datos 

beer$UPC <- replace(beer$UPC, beer$UPC==1820000016, 0)
beer$UPC <- replace(beer$UPC, beer$UPC==1820000106, 1)

# Seleccionar las variables necesarias 

beer <- select(beer,unidades="MOVE",precio="PRICE",marca="UPC")
head(beer)
```

una vez realizados los filtros necesarios, hacemos un cambio los tipos de datos con el fin de establecer el siguiente modelo:

$$

Demanda=\beta_{0}+\beta_{1}precio+\beta_{2}marca\\
\mbox{donde} \quad marca:=\text{Es una variable Dummy:}\\
\mbox{sí}:marca=0:\text{Budweiser beer} \quad marca=1:\text{Budweiser light beer}

$$


# Modelo - Estimación 

Para la estimación  del modelo planteado anteriormente vamos se define un vector para la demanda que representa la cantidad de unidades vendidas de cada producto, así mismo, se define una matriz de diseño que contiene, una columna de unos asociada al intercepto, otra columna de precios y tipo de marca definida como una variable Dummy.

En los siguientes apartados vamos a regresar diferentes modelos con el fin de revisar cual de los 3 ( Lin - Lin), (log -log) o (Log -Lig) se ajusta mejor a los datos.


## Modelo Lin - Lin

El primer modelo que vamos a revisar es un modelo lineal de forma matricial.

$$
Y=X\beta+e\\
\mbox{Donde} \quad Y=\text{Unidades por producto} \quad X=\text{Matriz diseño}\\
\quad\\

$$
Una vez realizada la estimación, los coeficientes asociados al modelo principal previamente mencionado, son los siguientes:


```{r}
y <- as.matrix(beer$unidades)
x <- as.matrix(cbind(1,beer$precio,beer$marca))
colnames(x) <-c("b0","precio","marca") 
betas <- (solve(t(x)%*%x)%*%t(x)%*%y)
betas
```
quedando de esta manera el siguiente modelo:

$$
\widehat{demanda}=59,33-12,90*Precio-3.16*Marca

$$
*Análisis:*
  - La demanda de Cervezas de la tienda 109 cuyas variables precio y marca, muestran una serie de comportamientos donde por un aumento el precio de 1und, la demanda disminuye 12,90 y cuando es 1  es decir "Budweiser beer", la demanda se reduce un -3,16. de otro modo es cero.


## Modelo Log - Lin

Ahora vamos a plantear un modelo Log-Lin y revisaremos los coeficientes asociados a cada variable explicativa de la demnada de cervezas de la tienda 109 de (DFF)

$$
ln(consumo)=\beta_{0}+\beta_{1}precio+\beta_{2}marca+e\\

$$
Dado que estamos trabajando las regresiones de forma matricial, antes de correr la regresión la variable consumo, representada en unidades vendidas se le hace una tranformación logaritmica y luego si se corre la respectiva regresión.



```{r}
y <- as.matrix(beer$unidades)
x <- as.matrix(cbind(1,beer$precio,beer$marca))
colnames(x) <-c("b0","precio","marca") 
lny <- log(y)
betas2 <- (solve(t(x)%*%x)%*%t(x)%*%lny)
betas2
```
interpretación:


## Modelo Log - Log


Ahora vamos a plantear un modelo Log-Log en donde nos aplicaremos una transformación logaritmica a los precios, pero no la a la variable dummmy dado que el log(0) no existe, por lo tanto sera un modelo log-log en el par de conusmo y precio, como se muestra en el siguiente planteamiento. 

$$
ln(consumo)=\beta_{0}+\beta_{1}ln(precio)+\beta_{2}marca+e\\

$$
Dado que las regresiones se estan trabajando de manera matricial, la matriz de diseño X y la matriz de Y "Consumo" se trabajaria de la siguiente manera:
$$


$$
Una vez realizamos la regresión, nos da los siguientes coeficientes asociados a cada variable.

```{r}
y <- as.matrix(beer$unidades)
logx <- as.matrix(cbind(1,log(beer$precio),beer$marca))
colnames(x) <-c("b0","precio","marca") 
lny <- log(y)

betas3 <- (solve(t(logx)%*%logx)%*%t(logx)%*%lny)
betas3
```
Interpretación:



# Matriz de Varianza Covarianza para los estimadores

Teniendo en cuenta lo anterior, notamos que hay diferentes interpretaciones para cada modelo, pero seria conveniente también conocer la variación de dichos coeficientes para cada modelo, por esta razón vamos a calcular la matriz de varianzas-covarianzas para los diferentes estimadores de cada modelo, para eso utilizaremos la siguiente formula:

$$
\widehat{var-cov(\hat\beta)}=\hat\sigma^2(X^tX)\\
\quad\\
\mbox{donde,} \quad \hat\sigma^2=\frac{\hat{e}^t\hat{e}}{T-K} \quad T=observaciones,\quad K=parámetros

$$
Teniendo en cuenta que la estimación de la varianza se realiza apartir de los errores estimados, en cada modelo vamos a realizar dicho procedimiento para ajustarlo al tipo de modelo.

## Modelo Lin - Lin

Antes de calcular la matriz de varianza covarianza vamos a estimar los errores 

$$
\hat{e}=Y-\hat{Y}
$$
```{r}
y <- as.matrix(beer$unidades)
x <- as.matrix(cbind(1,beer$precio,beer$marca))
colnames(x) <-c("b0","precio","marca") 
betas <- (solve(t(x)%*%x)%*%t(x)%*%y)
y_gorro <- x%*%betas
e_gorro1 <- y-y_gorro

# Calcular sigma^2
t <- as.numeric(nrow(y))
k <- as.numeric(ncol(x))
var1 <- as.numeric((t(e_gorro1)%*%e_gorro1)/(t-k))
var1

```
Una vez que tenemos calcula la varianza estimada, vamos a calcular nuestra matriz var_cov

```{r}
x <- as.matrix(cbind(1,beer$precio,beer$marca))
var_cov1 <- var1*(solve(t(x)%*%x))
var_cov1
```


## Modelo Log - Lin

Ahora tenermos que tener encuenta que tipo de modelo estamos estimando para así poder contruir nuestra matriz de var-cov, en donde a diferencia del anterior modelo, tenemos que hacer la siguiente modificación:
$$
ln(consumo)=\beta_{0}+\beta_{1}precio+\beta_{2}marca\\
\widehat{ln(consumo)}=X\hat{\beta} \quad \implies
\widehat{consumo}=e^{\widehat{ln(consumo)}}
$$
```{r}
y <- as.matrix(beer$unidades)
x <- as.matrix(cbind(1,beer$precio,beer$marca))
colnames(x) <-c("b0","precio","marca") 
lny <- log(y)
betas2 <- (solve(t(x)%*%x)%*%t(x)%*%lny)

log_y_gorro2 <- x%*%betas2
y_gorro2 <- exp(log_y_gorro2)
e_gorro2 <- y-y_gorro2

# Calcular sigma^2
t <- as.numeric(nrow(y))
k <- as.numeric(ncol(x))
var2 <- as.numeric((t(e_gorro2)%*%e_gorro2)/(t-k))
var2
```
```{r}
x <- as.matrix(cbind(1,beer$precio,beer$marca))
var_cov2 <- var2*(solve(t(x)%*%x))
var_cov2
```

## Modelo Log - Log


```{r}
y <- as.matrix(beer$unidades)
logx <- as.matrix(cbind(1,log(beer$precio),beer$marca))
colnames(x) <-c("b0","precio","marca") 
lny <- log(y)

betas3 <- (solve(t(logx)%*%logx)%*%t(logx)%*%lny)

log_y_gorro3 <- logx%*%betas3
y_gorro3 <- exp(log_y_gorro3)
e_gorro3 <- y-y_gorro3

# Calcular sigma^2
t <- as.numeric(nrow(y))
k <- as.numeric(ncol(x))
var3 <- as.numeric((t(e_gorro3)%*%e_gorro3)/(t-k))
var3
```
```{r}
logx <- as.matrix(cbind(1,log(beer$precio),beer$marca))
var_cov3 <- var3*(solve(t(logx)%*%logx))
var_cov3

```


# R ajustado de cada especificación 

Hasta el momento hemos realizado la estimación de los diferentes modelos, nos centramos en conocer sus diferentes matrices de varianzas - Covarianzas, pero no hemos revisado que tanto las variables explicativas, explican la variabilidad del consumo de cervezas para las marcas seleccionadas.

Para conocer dicha que tan ajustado es del modelo en términos de variabilidad utilizaremos el R^2 ajustado con la siguiente formula:

$$
R_{ajustado}^2=1-(1-R^2)*(\frac{n-1}{n-k-1})\\
\mbox{donde,} \quad R^2=1-\frac{\sum_{t=1}^Ne_{t}^2}{\sum_{t=1}^N(Y_t-\bar{Y})^2}

$$
Dado que tenemos diferentes modelo vamos a calcularlo para cada uno de ellos.

```{r}
R2ADJ <- function(y,e_gorro,n,k,R22){
  y <- as.matrix(y)
  e_gorro <- as.matrix(e_gorro)
  mean_y <- sum(y)/length(y)
  R_2 <- 1- (sum((e_gorro)^2)/(sum((y-mean_y)^2)))
  R2ADJ <- 1-(((n-1)/(n-k-1))*(1-R_2)) 
  return(R2ADJ)
}
n <- nrow(beer)
k <- ncol(x)
```

## Modelo Lin - Lin

El primer modelo que vamos a revisar es el de Lin-Lin en donde podemos evidenciar que la variabilidad del modelo explica en 0.3033 la variabilidad del consumo. 

```{r}
y <- as.matrix(beer$unidades)
x <- as.matrix(cbind(1,beer$precio,beer$marca))
R2ADJ(y = y,e_gorro = e_gorro1,n = n,k = k)

```

## Modelo Log - Lin

Por otro lado, tenemos el modelo LOg-lin en donde únicamente aplicamos una transformación logarítmica  a la variable del consumo, una vez calculado el R^2 ajustado notamos que el modelo explica en *0.261878*  la variabilidad del consumo, cabe resaltar que antes de calcular el consumo estimado para el cálculo de los errores se aplica exponente al logaritmo del consumo para dejarlo en términos lineales. 

```{r}
y <- as.matrix(beer$unidades)
x <- as.matrix(cbind(1,beer$precio,beer$marca))

R2ADJ(y = y,e_gorro = e_gorro2,n = n,k = k)

```

## Modelo Log - Log

Finalmente, procedemos a calcular el R^2 ajustado del modelo Log - Log, el cual nos dice que dicho modelo explica en 0.2598559 la variabilidad del consumo, cabe resaltar nuevamente que antes de calcular el consumo estimado para el cálculo de los errores se aplica exponente al logaritmo del consumo para dejarlo en términos lineales. 

```{r}
y <- as.matrix(beer$unidades)
x <- as.matrix(cbind(1,beer$precio,beer$marca))

R2ADJ(y = y,e_gorro = e_gorro3,n = n,k = k)

```
Teniendo en cuenta los resultados anteriores, podemos evidenciar que el modelo que mejor explica la variabilidad del consumo, es el modelo Lin - Lin.


# Distancia de Cook

Aunque la muestra es representativa y aleatoria, pueden existir valores que afecten el cálculo de los betas, por esta razón vamos a utilizar el teorema de Cook o distancia de cook, que nos mide la influencia de un dato (i) sobre los coeficientes.

$$
D_{i}=\frac{(\hat{\beta_{i}}-\hat{\beta})^tX^tX(\hat{\beta_{i}}-\hat{\beta})}{K*MCE}\\
\mbox{Donde,} \quad MCE=\frac{1}{T-K}\sum_{t=1}^T(y_t-\hat{y_t})^2

$$


```{r}
cook <- function(y,x){
  
  x <- as.matrix(x)
  y <- as.matrix(y)
  coefi <- solve(t(x)%*%x)%*%t(x)%*%y
  distancia <- c()
  t <- length(y)
  y_gorro <- x%*%coefi
  k <- ncol(x)
  MCE <- ((1/t-k)*sum((y-y_gorro)^2))
  
  for (i in 1:n) {

    xc <- x[-i,]
    yc <- y[-i,]
    coefi2 <- solve(t(xc)%*%xc)%*%t(xc)%*%yc
    distancia[i] <- (t((coefi2-coefi))%*%t(x)%*%x%*%(coefi2-coefi))/(k*MCE)
    xc <- x
    yc <- y 
  }
  
  tabla <- cbind(y,x,distancia)
  return(head(tabla))
  
}
y <- as.matrix(beer$unidades)
x <- as.matrix(cbind(1,beer$precio,beer$marca))
cook(y =y,x = x)

```

Una vez que calculamos las diferentes distancias para cada observación, se revisa a través de un filtro las distancias mayores a uno, en donde nos dio que ningún dato es mayor a uno, por lo tanto los datos que tenemos no son influyentes a la hora de estimar los betas. 


# Pronóstico 

Finalmente a través de nuestro modelo Lin-Lin vamos pronosticar la demanda que se tendrían con un precio de 2/3, sin embargo debemos tener en cuenta que la otra variable que explica la demanda es un avariable Dummy con la marca de cerveza en donde: 

$$
marca=0:\text{Budweiser beer} \\\quad marca=1:\text{Budweiser light beer}
$$
$$
\widehat{demanda}=59,33-12,90*Precio-3.16*Marca\\
\mbox{si: Precio =2/3 & Marca=0}\\
\quad\\
\widehat{demanda}=59,33-12,90*(2/3)-3.16*(0)\\
\widehat{demanda}=50.73\\
\quad\\
\mbox{si: Precio =2/3 & Marca=1}\\
\widehat{demanda}=59,33-12,90*(2/3)-3.16*(1)\\
\widehat{demanda}=47.57\\


$$


A lo que podemos concluir que ante un precio alto la demanda disminuye cuando se trata de la cerveza tipo Budweiser light beer, a diferencia de la otra cerveza.


